{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "**ANSWER:** Information Gain tells us how much “information” a feature gives us about the target variable.\n",
        "\n",
        "It is based on Entropy, which measures impurity (randomness).\n",
        "\n",
        "- Entropy formula:\n",
        "\n",
        "$$Entropy(S) = - \\sum p_i \\log_2(p_i)$$\n",
        "\n",
        "Where $p_i$ proportion of class $i$ n dataset $S$.\n",
        "\n",
        "- Information Gain formula:\n",
        "\n",
        "$$Entropy(S) = - \\sum_{i} p_i \\log_2(p_i)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $S$= parent dataset\n",
        "\n",
        "- $A$= feature\n",
        "\n",
        "- $S_v$= subsets formed by splitting on feature $A$"
      ],
      "metadata": {
        "id": "4V1LF1Zogw1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "**ANSWER:** Here’s a simple and clear explanation of the difference between Gini Impurity and Entropy, both used in Decision Trees to measure how mixed a node is:\n",
        "\n",
        "- Gini Impurity vs Entropy:\n",
        "\n",
        "1. Definition:\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "- Measures the probability of incorrectly classifying a randomly chosen element.\n",
        "\n",
        "- Formula:\n",
        "\n",
        "$$Gini = 1 - \\sum p_i^2$$\n",
        "\n",
        "- Where $p_i$ = probability of class i.\n",
        "\n",
        "Entropy\n",
        "\n",
        "- Measures the amount of randomness or disorder in the dataset.\n",
        "\n",
        "- Formula:\n",
        "\n",
        "$$Entropy = - \\sum p_i \\log_2(p_i)$$\n",
        "\n",
        "2. Range:\n",
        "\n",
        "| Metric      | Minimum | Maximum             | Meaning of Maximum       |\n",
        "| ----------- | ------- | ------------------- | ------------------------ |\n",
        "| **Gini**    | 0       | 0.5 (for 2 classes) | Classes are evenly split |\n",
        "| **Entropy** | 0       | 1 (for 2 classes)   | Classes are evenly split |\n",
        "\n",
        "3. Interpretation:\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "i. Faster to compute.\n",
        "\n",
        "ii. More sensitive to node purity.\n",
        "\n",
        "iii. Often prefers larger partitions.\n",
        "\n",
        "Entropy\n",
        "\n",
        "i. More computationally expensive due to logarithm.\n",
        "\n",
        "ii. Values increase more smoothly.\n",
        "\n",
        "iii. Used in ID3, C4.5 algorithms."
      ],
      "metadata": {
        "id": "h16mKuaHizlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "**ANSWER:** Pre-pruning (also called early stopping) is a technique in Decision Trees where you stop the tree from growing further during the training process to prevent it from becoming too large and overfitting the data.\n",
        "\n",
        "Pre-pruning is the process of limiting the growth of a decision tree by applying stopping conditions before a split is made.\n",
        "\n",
        "It avoids unnecessary splits that do not significantly improve model performance.\n",
        "\n",
        "A fully grown tree:\n",
        "\n",
        "- becomes too complex\n",
        "\n",
        "- learns noise from training data\n",
        "\n",
        "- causes overfitting\n",
        "\n",
        "- reduces generalization on test data\n",
        "\n",
        "Pre-pruning keeps the tree simple and effective.\n",
        "\n",
        "Common Pre-Pruning Techniques:\n",
        "\n",
        "You can stop the tree from splitting based on:\n",
        "\n",
        "1. Maximum Depth -\n",
        "\n",
        "Stop splitting once the tree reaches a certain depth.\n",
        "\n",
        "2. Minimum Samples Split -\n",
        "\n",
        "Do not split a node if it contains fewer samples than required.\n",
        "\n",
        "3. Minimum Samples per Leaf -\n",
        "\n",
        "Each leaf must contain at least a set number of samples.\n",
        "\n",
        "4. Minimum Impurity Decrease -\n",
        "\n",
        "Only split if impurity decreases by a sufficient amount."
      ],
      "metadata": {
        "id": "eXDcP65dGMxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. :Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "**ANSWER:**"
      ],
      "metadata": {
        "id": "do_Td7cdHFDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Decision Tree Classifier using Gini impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lief66hPHzWz",
        "outputId": "2a6d85d5-9ef7-40dc-b03a-910045751279"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "**ANSWER:** A Support Vector Machine finds the best possible boundary (called a hyperplane) that separates data into different classes with the maximum margin.\n",
        "\n",
        "1. Hyperplane:\n",
        "\n",
        "A line (in 2D) or a plane (in higher dimensions) that separates classes.\n",
        "\n",
        "2. Margin:\n",
        "\n",
        "The distance between the hyperplane and the nearest data points from each class.\n",
        "SVM chooses the hyperplane with the maximum margin → better accuracy and generalization.\n",
        "\n",
        "3. Support Vectors:\n",
        "\n",
        "The important data points that lie closest to the hyperplane.\n",
        "They “support” or define the decision boundary.\n",
        "\n",
        "4. Kernel Trick:\n",
        "\n",
        "Allows SVM to classify non-linear data by mapping it into higher dimensions.\n",
        "Common kernels:\n",
        "\n",
        "- Linear\n",
        "\n",
        "- Polynomial\n",
        "\n",
        "- RBF (Radial Basis Function)\n",
        "\n",
        "- Sigmoid"
      ],
      "metadata": {
        "id": "BZtCsVknH2b6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the Kernel Trick in SVM?\n",
        "\n",
        "**ANSWER:** The Kernel Trick in Support Vector Machines (SVM) is a mathematical technique that allows SVMs to separate data that is not linearly separable in its original space by implicitly mapping it to a higher-dimensional feature space — without actually performing the transformation.\n",
        "\n",
        "Instead of transforming data to a higher dimension explicitly (which is expensive or impossible), the kernel trick uses a kernel function to compute the dot product in the higher-dimensional space directly.\n",
        "\n",
        "This lets SVM draw non-linear decision boundaries efficiently.\n",
        "\n",
        "- Kernel Trick Needed:\n",
        "\n",
        "Some datasets cannot be separated by a straight line (linear boundary).\n",
        "Example: XOR pattern, spirals, curved shapes, etc.\n",
        "\n",
        "Mapping to a higher dimension can make data linearly separable.\n",
        "\n",
        "But explicitly computing this mapping is:\n",
        "\n",
        "I. costly\n",
        "\n",
        "II. sometimes infinite-dimensional\n",
        "\n",
        "III. mathematically complex\n",
        "\n",
        "A kernel function $K(x,x')$ satisfies:\n",
        "\n",
        "$$K(x, x') = \\phi(x) \\cdot \\phi(x')$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $\\phi(x)$ =implicit transformation to high dimension\n",
        "\n",
        "- You never compute $\\phi(x)$\n",
        "\n",
        "- You only compute $K(x,x')$"
      ],
      "metadata": {
        "id": "vScKYbmhIWZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "**ANSWER:**"
      ],
      "metadata": {
        "id": "WDlBDJhsK46d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Standardize features (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "linear_pred = svm_linear.predict(X_test)\n",
        "linear_acc = accuracy_score(y_test, linear_pred)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "rbf_pred = svm_rbf.predict(X_test)\n",
        "rbf_acc = accuracy_score(y_test, rbf_pred)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"Accuracy of SVM with Linear Kernel:\", linear_acc)\n",
        "print(\"Accuracy of SVM with RBF Kernel:\", rbf_acc)\n",
        "\n",
        "if rbf_acc > linear_acc:\n",
        "    print(\"\\nRBF kernel performed better.\")\n",
        "elif rbf_acc < linear_acc:\n",
        "    print(\"\\nLinear kernel performed better.\")\n",
        "else:\n",
        "    print(\"\\nBoth kernels performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV5nC-EqLIYS",
        "outputId": "73ea2a40-de3f-4d02-8f35-37e7ca316d08"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 0.9814814814814815\n",
            "Accuracy of SVM with RBF Kernel: 0.9814814814814815\n",
            "\n",
            "Both kernels performed equally well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "**ANSWER:** The Naïve Bayes classifier is a simple yet powerful probabilistic machine learning algorithm based on Bayes’ Theorem. It is often used for classification tasks like spam detection, sentiment analysis, document classification, email filtering, and more.\n",
        "\n",
        "The Naïve Bayes classifier predicts the class of a data point using:\n",
        "\n",
        "$$P(\\text{Class} \\mid \\text{Features}) =\n",
        "\\frac{P(\\text{Features} \\mid \\text{Class}) \\cdot P(\\text{Class})}\n",
        "{P(\\text{Features})}$$\n",
        "\n",
        "It calculates the probability of each class given the input features and chooses the class with the highest probability.\n",
        "\n",
        "In simple words:\n",
        "\n",
        "- It learns how likely each class is (prior probability)\n",
        "\n",
        "- It learns how each feature behaves within each class (likelihood)\n",
        "\n",
        "- It combines these probabilities using Bayes’ theorem and predicts the most probable class\n",
        "\n",
        "It is called Naïve because it makes a very strong and unrealistic assumption:\n",
        "\n",
        "- All features are independent of each other given the class.\n",
        "\n",
        "This means Naïve Bayes assumes:\n",
        "\n",
        "i. Every feature contributes independently to the outcome\n",
        "\n",
        "ii. There is no correlation between features\n",
        "\n",
        "For example, when classifying emails:\n",
        "\n",
        "i. The words “free,” “money,” and “win” are clearly related\n",
        "\n",
        "ii. But Naïve Bayes treats them as independent features\n",
        "\n",
        "This assumption is almost never true in real-world data, hence the name:\n",
        "\n",
        "“Naïve” = Overly simple assumption of feature independence:\n",
        "\n",
        "Yet, despite this unrealistic assumption:\n",
        "\n",
        "- Naïve Bayes works very well in practice\n",
        "\n",
        "- It is fast, efficient, and surprisingly accurate for many applications\n",
        "\n",
        "- It performs especially well for text classification and high-dimensional data."
      ],
      "metadata": {
        "id": "j3dWvu8HLNPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "| **Aspect**               | **Gaussian Naïve Bayes**                         | **Multinomial Naïve Bayes**             | **Bernoulli Naïve Bayes**                   |\n",
        "| ------------------------ | ------------------------------------------------ | --------------------------------------- | ------------------------------------------- |\n",
        "| **Type of Data Used**    | Continuous numerical data                        | Discrete counts                         | Binary data (0/1)                           |\n",
        "| **Feature Examples**     | Height, weight, temperature                      | Word counts, term frequencies           | Word present/not present                    |\n",
        "| **Assumed Distribution** | Gaussian (Normal distribution)                   | Multinomial distribution                | Bernoulli (binary) distribution             |\n",
        "| **Input Value Type**     | Real numbers (can be negative)                   | Non-negative integers                   | 0 or 1                                      |\n",
        "| **Common Use Cases**     | Sensor data, continuous measurements, image data | Text classification using word counts   | Text classification using binary features   |\n",
        "| **NLP Usage**            | Less common                                      | Very common                             | Common for binary text models               |\n",
        "| **When to Use**          | When features are continuous                     | When features represent counts          | When features represent presence/absence    |\n",
        "| **Output**               | Probability based on mean and variance           | Probability based on counts of features | Probability based on binary feature matches |\n"
      ],
      "metadata": {
        "id": "6fatJquUSVdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "**ANSWER:**"
      ],
      "metadata": {
        "id": "kejkxoRSTOwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create and train Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gaussian Naive Bayes Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YhGUIZNTh10",
        "outputId": "03e0d871-6293-417a-e042-d0d2d53987e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naive Bayes Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}